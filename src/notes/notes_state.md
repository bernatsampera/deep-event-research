How to organize the State.

Now we have

- messages, list of message
- events, list of json objects
- person to reserach

What could we add.

- events summary. Contains a brief description of each event with the id like this.

  - 1.  Birth of H. Miller
  - 23. Write Tropic of cancer

This would match the events json

# { -->

# "events": [

# {

# "id": "1",

# "name": "Birth of Henry Miller",

# "description": "Henry Miller was born in USA on 1990-01-01 in a hospital in Bushwick, Brooklyn",

# },

# {

# "id": "23",

# "name": "Wrote Tropic of Cancer",

# "description": "Henry Miller, wrote tropic of cancer, this is a book about his complications and misfortunies",

# }

# ]

# }

Could we just use the name instead of events summary? Or would it be practic to have it to update it faster and be able to pass the whole file
without parsing it?

When is event_summary generated?

It could be generated by the url_crawler and report a summary of the events.

Url crawler --> generates events_summar

We have list of events

How to generate events json structured?
Automatically after each url_crawler (A la workflow). We can also keep it in the tool then right? Or maybe not?
Maybe outside the tool is better for separation of concerns.

Then we would have

1. Url_crawler (url) -> List of events in txt
   1b. create/merge structured events json.

2. Tool chooser
   Url finder (can be also a query about other things that the biography, like a specific event or something like that)
   (should we keep track of the urls that have already been parsed?)

Ideas?
Create summary of the messages so we are not passing all the messages?

Remove event_further_research and make that the url finder is not just about the biography but the following.

1. Think_tool, decides which will be the query made to the search engine

EX: General biography of Henry Miller
Ex: Why did Henry Miller went to France?

Urls are returned [2,4] (discard urls that have already been used, keep track of them in the state)

2. Go in a loop through all the urls that have not been retrieved yet and from here retrieve the events_summary for each url
   From the events summary, see if each event has an equivalent in the events.json and if so, this will be added. Each event has a date (in years, even if it's not exact, to order it, besides the ID)

If there is an requivalent, create a prompt with new information plus old event.
If not, add it to the events list with a date and then it will be sorted.

3. Go again to the think_loop and based on the events_summary, create a new prompt for the search engines.
   Would be nice a way to evaluate the search engine response and which urls are more relevant for our case

Next Steps

1. Remove even further research DONE
2. Create the url finder tool and see which urls are returned based on the events.
